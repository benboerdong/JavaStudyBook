### 分布式锁分享

# 分布式锁分享

## **1.分布式锁的解释**

### 1.1什么是分布式

**【Distributed Systems Concepts and Design】**（分布式系统概念与设计**）一书中分布式解释：**

*分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间通过消息传来递进行通讯和协调的系统。*

**分布式的特点：**

- **分布性**

  分布式系统中的多台计算机之间在空间位置上可以随意分布，系统中的多台计算机之间没有主、从之分，即没有控制整个系统的主机，也没有受控的从机。

- **协同性**

  系统中的若干台计算机可以互相协作来完成一个共同的任务，或者说一个程序可以分布在几台计算机上并行地运行，提升应用的执行效率。

- **可扩展性**

  企业级应用平台必须要能适应需求的变化，随着并发请求的增加，可以弹性地增加服务器的数量。

- **通信性**

  分布式系统中的任意两台计算机都能通信，进行信息交换。

**CAP理论和BASE理论：**

CAP ：1.一致性（Consistency）、2.可用性（Availablity）、3.分区容忍性（Partition-torlerance）。

BASE： 基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。

### 1.2分布式锁概念

#### **分布式锁出现背景**

随着公司业务增长，应对高并发场景不得不使用分布式集群，为了保证数据的一致性，需要一些技术手段实现包括**分布式事务和分布式锁。**单机情况下的多线程可以共享堆内存，通过在内存中标记变量实现互斥，我们可以使用Java自带的锁来实现互斥，但是在分布式场景下，多个客户端无法共享内存，为了解决数据一致性问题，需要将标记一个变量也就是锁，放在全局的位置来实现互斥，这样就出现了分布式锁。

#### **分布式锁实现要求**

- 可以保证在分布式集群中，同一时刻只有一个客户端获得锁。

- 这把锁要是一把可重入锁（避免死锁）

- 这把锁最好是一把阻塞锁

  阻塞锁让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，可以进入就绪状态，就绪状态中所有线程通过竞争，进入运行状态。

- 这把锁最好是一把公平锁

  公平锁就是线程按照执行顺序依次获取锁，好处是可以公平的让线程获取锁，避免有线程等待时间过长获取不到锁的情况，但是这种方式也有弊端：额外维护排队逻辑，额外损耗性能。

- 有高性能、高可用的获取锁和释放锁功能

## **2.数据库实现分布式锁**

**实现原理**

通过向数据表中添加记录表示加锁，删除表中记录表示释放锁

**大体实现**

加锁：先select 然后执行 insert or update

释放锁：delete from contentMethodLock where resource_name=XXX and client_info= XXX



![img](https://app.yinxiang.com/FileSharing.action?hash=1/c1cb1e438948c6ecdda3f754f4d08b18-140480)



#### **2.1悲观锁实现**

每次在拿取数据的时候都会对数据上锁，这样其他线程想用这个数据就会阻塞，直到获取锁的线程释放锁，例如MySQL的行锁和表锁。

可以使用selectfor update来实现，数据库会在查询过程中给数据库表加上排他锁

**添加锁**

代码块

SQL

select * from contentMethodLock where client_info ='asdfb12345' and resource_name = 'order_in_stock' for update

InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁

**释放锁**

代码块

SQL

connection.commit() //这种方式需要显示的去提交事务，不然无法释放锁，导致锁一直占用

#### **2.2 乐观锁实现**

总是假设最好的情况，每次去拿数据的时候都认为不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间线程有没有去更新这个数据，可以使用版本号机制实现。

实现思想：先在数据表中添加对应的版本号字段，每当数据库中进行数据更新时，给对应的版本号加1。简而言之，每次进行数据更新时，先要比较版本号，然后判定是否需要进行更新操作。



![img](https://app.yinxiang.com/FileSharing.action?hash=1/7c6f64233660913ee7ab645b42c4cf51-260937)



代码块

select version from distributed_lock where resource_name = 'order_in_stock';

update distributed_lock set client_info='asdfb12345',version=version+1

where resource_name = 'order_in_stock' and version=${version};

**优劣分析**

- 优点

- - 逻辑简单，便于理解
  - 除了mysql，不用借助外部的组件缓存，zk等

- 不足

- - 性操作数据库所以性能差，并发高了会有诸多问题
  - 满足不了高可用，有单节点问题，即使通过主-备的形式来缓解，由于主从同步比较慢，但是这样会引来数据不一致的问题。
  - 得自己实现锁的阻塞，重入等。

## **3.基于缓存的分布式锁**

### 3.1原生Redis实现

#### **实现原理**

**加锁**

（1）判断加锁是否超时，如果没有超时执行setnx(key,expireTime),如果加锁成功则执行业务，如果没有没有加锁成功进入下一步。

（2）get(key)获取过期时间expireTime，判断expireTime是否小于当前时间，如果不小于说明锁未过期，则继续返回加锁流程，如果expireTime小于当前时间说明锁已过期进入下一步。

（3）当锁过期说明 可以加锁了，执行getSet(key, 新expireTime)操作得到oldExpireTime。

（4）此时判断oldExpireTime是否等于expireTime返回值，如果不相等说明有其他客户端已经加锁了，此时继续返回加锁流程。如果oldExpireTime等于expireTime返回值 说明其他客户端没有加锁，则获取到锁

释放锁：

在业务执行完后，直接调用Redis中的del()方法。但是需要注意的是加锁和释放锁需要引入一个客户端标识，否则会有误删其他客户端锁情况。具体情况如下：

（1）进程开始获取锁成功，得到锁lockA

（2）A进程在释放锁的时候，执行del()操作之前，锁lockA正好过期了，进程B得到了锁，并设置成功了lockB

（3）A进程在执行del()操作时，会导致删除锁B

#### **优劣分析**

- 优点

- - 对缓存的操作，速度快，性能好
  - 能满足高并发的需求

- 不足

- - 实现逻辑较复杂，开发需考虑各种问题
  - 不支持可重入，公平锁，当然可以自己实现
  - 如果一个节点或集群有问题，主从复制数据没完成，导致k-v丢失,造成多个客户端获取锁

**公司组件**

[分布式锁-基于redis的squirrel的实现](https://km.sankuai.com/page/62051194#id-基于公司squirrel实现)

[Cellar分布式锁实现方案](https://km.sankuai.com/page/116534640)

[Cerberus-分布式锁](https://km.sankuai.com/page/204298409)

### 3.2 Redisson实现

Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务。Redisson在基于NIO的[Netty](https://baike.baidu.com/item/Netty)框架上，充分的利用了Redis键值数据库提供的一系列优势，在Java实用工具包中常用接口的基础上，为使用者提供了一系列具有分布式特性的常用工具类。总之，Redisson提供了使用Redis的最简单和最便捷的方法，从而让使用者能够将精力更集中地放在处理业务逻辑上。

通过Redssion实现分布式锁的大致方式如下，使用Redisson方式很简单，因为内部已经封装完善的分布式锁的功能，包扩但不仅限于自动续期机制、锁的可重入、公平锁等机制

代码块

Java

RedissonClient redisson = Redisson.create(config);

RLock lock = redisson.getLock("anyLock");

boolean isLock;

try {

//waitTime 超时时间，leaseTime加锁时间

isLock = lock.tryLock(waitTime, leaseTime, TimeUnit.SECONDS);

if (isLock) {

//TODO 业务逻辑

Thread.sleep(3000);

}

} catch (Exception e) {

} finally {

// 无论如何, 最后都要解锁

redLock.unlock();

}

#### **实现原理**

**实现流程图（有误修改中）**



![img](https://app.yinxiang.com/FileSharing.action?hash=1/dd7758eabcf5fbba114ef2d5624fef96-615247)



**加锁逻辑**

（1）执行判断操作，判断lock锁key是否存在，不存在直接调用hset存储当前线程信息并且设置过期时间,返回成功告诉客户端直接获取到锁,注意是一段lua脚本是原子操作。

为什么lua脚本 + redis命令是原子操作：因为lua脚本连续执行，再加上redis是单线程，所以lua脚本里执行redis命令具有排他性。



![img](https://app.yinxiang.com/FileSharing.action?hash=1/bd21244c3f0d11416f711c4f553cd2c6-382040)



代码块

Java

//lock的hash存储结构 guid + 当前线程的ID。后面的value是就和可重入加锁次数

myLock:{

"8743c9c0-0795-4907-87fd-6c719a6b4586:1": 1

}

（2）如果lock锁key存在，获取lock锁key的hash数据结构，判断guid + 当前客户端线程的ID 是否和当前加锁线程能对应上，如果能对应上则将重入次数加1，并重新设置过期时间，返回成功，告诉客户端直接获取到锁。

（3）如果 myLock数据结构和当前线程信息对应不上，客户端2线程会获取到pttl myLock返回的一个数字，这个数字代表了myLock这个锁key的**剩余生存时间。**比如还剩15000毫秒的生存时间。此时客户端2线程会进入一个while循环，不停的尝试加锁。

**解锁逻辑**

（1）由于支持可重入，在解锁时将重入次数需要减1，如果发现加锁次数是0了，说明这个客户端已经不再持有锁了，此时就会用："del myLock"命令，从redis里删除这个key,解锁成功。

（2）如果锁不是被当前线程锁定或者lock锁key不存在，则返回成功。

**watch dog自动延期机制**

客户端1加锁的锁key默认生存时间才10秒，如果超过了10秒，客户端的业务逻辑还没执行完，怎么办呢？那就用到watch dog自动延期机制

获取锁成功就会开启一个定时任务,也就是watchdog,定时任务会定期检查去续期renewExpirationAsync(threadId).通过源码分析我们知道,默认情况下,加锁的时间是30秒.如果加锁的业务没有执行完,那么到 30-10 = 20秒的时候,就会进行一次续期,把锁重置成30秒。

[03 - squirrel下watchdog机制实现](https://km.sankuai.com/page/261093323)

**读写锁**

代码块

JSON

//读锁结构

anyLock: {

"mode": "read",

"UUID_01:threadId_01": 2,

"UUID_02:threadId_02": 1

}

//写锁结构

anyLock: {

"mode": "write",

"UUID_01:threadId_01:write": 2

}

#### **优劣分析**

- 优点

- - 实现逻辑简单，开发量小
  - 支持锁的种类较多，包括可重入锁、公平锁、 红锁、读写锁和联锁。

- 不足

- - Redisson底层依赖Redis集群，公司现有的Squirrel(依赖Jedis的api)，而Redisson自己实现操作redis底层API，无法集成到Squirrel。（但是没关系Squirrel的也有这些锁的实现，原理基本相同）
  - 在Redis master宕机的时候，可能导致多个客户端同时完成加锁

[01 - Squirrel引擎下的multiLock实现](https://km.sankuai.com/page/230963467)

### 3.3 redlock

#### **实现原理**

（1）Client使用相同的key和随机数,按照顺序在每个Master实例中尝试获得锁。在获得锁的过程中，为每一个锁操作设置一个**快速失败时间**(如果想要获得一个10秒的锁， 那么每一个锁操作的失败时间设为5-50ms)。

（2）客户端计算出与master获得锁操作过程中消耗的时间，当且仅当Client获得锁消耗的时间小于锁的存活时间，并且在N/2+1以上的master节点中获得锁。才认为client成功的获得了锁。

（3）如果Client获得锁的数量不足一半以上，或获得锁的时间超时，那么认为获得锁失败。客户端需要尝试在所有的master节点中释放锁， 即使在第二步中没有成功获得该Master节点中的锁，仍要进行释放操作。



![img](https://app.yinxiang.com/FileSharing.action?hash=1/8ff0d3b7d70bb756022b3d34947e18a9-296344)



#### **优劣分析**

- 优点

- - 完美解决Redis master宕机的时候，可能导致多个客户端同时完成加锁
  - 可用性高，即使有master节点挂了也完全可用

- 不足

- - Redisson底层依赖Redis集群，公司现有的Squirrel(依赖Jedis的api)，而Redisson自己实现操作redis底层API，无法集成到Squirrel。
  - redlock算法对时钟依赖性太强，若Ｎ个节点中的发生时间跳跃，导致过期，超时时间计算的问题。

思考：jvm fullgc STW会导致线程执行暂停，有client 1发生STW ,停顿的时间超过了锁的超时时间，然后client 2 获取了锁，此时client 1停顿结束了，还是会接着执行完锁住的那段代码，这样就出现了2个client同时获取了锁。怎么解决这个问题？

## **4.基于zookeeper的分布式锁**

ZooKeeper 是以 Paxos 算法为基础的分布式应用程序协调服务。ZK 的数据节点和文件目录类似，所以我们可以用此特性实现分布式锁。

### 4.1 原生Zookeeper

#### **实现原理**

**实现流程**

（1）客户端调用create()方法创建名为“*Lock*/node”的节点，需要注意的是，这里节点的创建类型需要设置为EPHEMERAL_SEQUENTIAL（临时排序节点）。

（2）客户端调用getChildren(“*Lock*”)方法来获取所有已经创建的子节点。

（3）客户端获取到所有子节点路径之后，如果发现自己在步骤1中创建的节点是所有节点中序号最小的，那么就认为这个客户端获得了锁。

（4）如果创建的节点不是所有节点中需要最小的，那么则监视比自己创建节点的序列号小的最大的节点，进入等待。直到下次监视的子节点变更的时候，再进行子节点的获取，判断是否获取锁。

**流程图**

#### **优劣分析**

- 优点

- - 锁模型健壮，可靠性高，客户端宕机了也没关系，zk感知到那个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁

- 不足

- - 效率较低，性能较差，通过频繁的创建节点和删除节点来实现加锁和解锁
  - 需要自己手动实现整个加锁和解锁操作，这样要考虑到各种细节，异常处理等。

### 4.2基于开源Curator客户端

Curator是一个比较完善的ZooKeeper客户端框架，通过封装的一套API 简化了ZooKeeper的操作，解决了很多Zookeeper客户端非常底层的细节开发工作。

#### **实现原理和使用**

实现逻辑和原生Zookeeper保持一致，如下第三方库[Curator](https://curator.apache.org/)客户端具体实现流程图

#### **可重入锁的实现原理**

每个InterProcessMutex实例，都会持有一个ConcurrentMap类型的threadData对象，以线程对象作为Key，以LockData作为Value值。通过判断当前线程threadData是否有值，如果有，则表示线程可以重入该锁，于是将lockData的lockCount进行累加；如果没有，则进行锁的抢夺。

internals.attemptLock方法返回lockPath!=null时，表明了该线程已经成功持有了这把锁，于是乎LockData对象被new了出来，并存放到threadData中。



![img](https://app.yinxiang.com/FileSharing.action?hash=1/bf4063b05485684865d674d553b32d2e-173420)



#### **优劣分析**

- 优势

- - 支持锁的种类较多，包括可重入锁，不可重入锁，公平锁， 读写锁，可根据具体业务场景配套使用

- 不足

- - 底层依赖Zookeeper集群，需要结合公司现有的Zookeeper集群，目前Lion实现就是通过Curator客户端操作Zookeeper

### 4.3基于公司内部Lion

Lion提供的分布式锁完全基于ZK，并且支持重入锁和读写锁，具体使用 实现见：[Lion分布式锁](https://km.sankuai.com/page/28252571#id-2.7.分布式锁)

Lion提供的分布式锁完全基于ZK，ZK的分布式锁不适用于高QPS是，锁的量很大的场景。

如果你的**锁QPS超过100**或者**锁的数量超过50万**，不要使用基于ZK的分布式锁，应寻求基于Redis的分布式锁方案。

摘自[Lion 使用文档](https://km.sankuai.com/page/28252571#id-2.7.分布式锁)

#### **优劣分析**

- 优势

- - 结合公司现有的能力，快速实现分布式锁
  - 高可用得到保障，有对应的集群

- 不足

- - 受zookeeper性能约束，不适合高并发场景

## **5.基于Cerberus**

Cerberus是到店部门研发的分布式锁组件，目前提供了三种引擎供大家使用，Tair、zk和Squirrel。如果有需要，除主引擎外，也可配置另外1-2种副引擎，以便必要时进行降级处理。

[01 - 接入指南](https://km.sankuai.com/page/204344612) [Cerberus-分布式锁](https://km.sankuai.com/page/204298409)

#### **实现原理**

整体架构



![img](https://app.yinxiang.com/FileSharing.action?hash=1/58988f19782b533c05716334c30e4558-233376)



**现状**

|                      |                                                              |                                                              | ZooKeeper                                                    | Cellar                                                       | Squirrel |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- |
| 实现方式             | 基本使用Menagerie源码，有细微改动                            | 利用Cellar的expirelock，仿照Menagerie和Concurrent包开发      | 基于redis集群                                                |                                                              |          |
| 功能                 | 可自主选择引擎                                               | 实现                                                         | 实现                                                         | 实现                                                         |          |
| 可重入锁             | lock()                                                       | 实现                                                         | 实现                                                         | 实现                                                         |          |
| tryLock()            | 实现                                                         | 实现                                                         | 实现                                                         |                                                              |          |
| lockInterruptibly()  | 实现                                                         | 实现                                                         | 实现                                                         |                                                              |          |
| unlock()             | 实现                                                         | 实现                                                         | 实现                                                         |                                                              |          |
| 公平/非公平锁        | 公平锁                                                       | 非公平锁                                                     | 非公平锁                                                     |                                                              |          |
| 可重入读写锁         | 实现                                                         | 未实现                                                       | 未实现                                                       |                                                              |          |
| Condition            | 实现                                                         | 未实现                                                       | 未实现                                                       |                                                              |          |
| MultiLock            | 未实现                                                       | 未实现                                                       | 实现                                                         |                                                              |          |
| WatchDog续租         | 未实现                                                       | 未实现                                                       | 实现                                                         |                                                              |          |
| 引擎故障检查         | 未实现                                                       | 未实现                                                       | 未实现                                                       |                                                              |          |
| 切换引擎             | 可通过接口手动切换引擎（切换引擎会导致锁丢失）               |                                                              |                                                              |                                                              |          |
| 用户可自定义集群环境 | 实现                                                         | 实现                                                         | 实现                                                         |                                                              |          |
| 羊群效应             | 无                                                           | 无                                                           | 无                                                           |                                                              |          |
| 集群                 | 线上                                                         | 已提供**host**=cerberus-zk.vip.sankuai.com**baseLockPath**=/dlm-locks | 已提供**remote_appkey**=com.sankuai.tair.hotel.plat.publicarea=22 | 已提供**clusterName**=redis-hotel-cerberus_product**category**=travel_cerberus |          |
| 线下                 | 已提供**host**=mobile-zk.test.sankuai.com:2181**baseLockPath**=/dlm-locks | 已提供**remote_appkey**=com.sankuai.tair.qa.functionarea=74  | 已提供**clusterName**=redis-hotel-cerberus_dev**category**=travel_cerberus |                                                              |          |

**测试报告：**

[cerberus综合测试](https://km.sankuai.com/page/59959712)

#### **降级方法**

当前使用的集群出现故障时，服务的负责人可在octo的mcc配置页进行手动切换。

将Key：cerberus_engine的值改为tair，点击保存。当前服务的引擎则会切换到tair。

如果改为其他值或者留空，则会按照Cerberus配置的引擎顺序，切换到下一个引擎

切换引擎会导致锁丢失

#### **优劣分析**

- 优势

- - 接入简单
  - 轻量级，java包形式
  - 支持多种底层引擎，根据业务需求来切换
  - 支持分布式锁降级处理

- 不足

- - 劣势就是各引擎的劣势

## **6.总结**

**对比**

| 数据库   | 缓存 | Zookeeper |       |
| -------- | ---- | --------- | ----- |
| 理解角度 | ✨✨✨  | ✨✨✨✨      | ✨✨✨✨  |
| 实现角度 | ✨✨✨  | ✨✨✨✨✨     | ✨✨✨✨  |
| 性能角度 | ✨✨✨  | ✨✨✨✨✨     | ✨✨✨✨  |
| 安全角度 | ✨✨✨  | ✨✨✨✨      | ✨✨✨✨✨ |

**方案倾向**

推荐使用公司的Cerberus作为分布式锁实现

### 资料参考

[01 - Squirrel引擎下的multiLock实现](https://km.sankuai.com/page/230963467)

[03 - squirrel下watchdog机制实现](https://km.sankuai.com/page/261093323)

[Lion 使用文档](https://km.sankuai.com/page/28252571#id-2.6.分布式锁)

[Cerberus-分布式锁](https://km.sankuai.com/page/204298409)

[01 - 接入指南](https://km.sankuai.com/page/204344612)

[Cerberus-分布式锁](https://km.sankuai.com/page/204298409)

https://km.sankuai.com/page/28252571#id-2.7.分布式锁

http://zhangtielei.com/posts/blog-redlock-reasoning.html

http://zhangtielei.com/posts/blog-redlock-reasoning-part2.html

https://diaozxin007.github.io/2017/10/29/Redis-RedLock-完美的分布式锁么？